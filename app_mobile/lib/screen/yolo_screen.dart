import 'dart:io';
import 'package:app_mobile/screen/emotion_result_screen.dart';
import 'package:flutter/material.dart';
import 'package:camera/camera.dart';
import 'package:path_provider/path_provider.dart';
import 'package:ultralytics_yolo/ultralytics_yolo.dart';
import 'package:image/image.dart' as img;

class YoloScreen extends StatefulWidget {
  const YoloScreen({super.key});

  @override
  State<YoloScreen> createState() => _YoloScreenState();
}

class _YoloScreenState extends State<YoloScreen> {
  int _count = 0;
  String _info = '';
  CameraController? _cameraController;
  List<YOLOResult>? _lastResults;
  bool _isCapturing = false;

  @override
  void initState() {
    super.initState();
    _initCamera();
  }

  Future<void> _initCamera() async {
    final cameras = await availableCameras();
    if (cameras.isEmpty) return;

    _cameraController = CameraController(
      cameras.first,
      ResolutionPreset.high,
      enableAudio: false,
    );

    await _cameraController!.initialize();
    if (mounted) setState(() {});
  }

  /// Ch·ª•p ·∫£nh v√† crop khu√¥n m·∫∑t v·ªõi scale ƒë√∫ng
  Future<void> _captureAndAnalyzeFace() async {
    if (_cameraController == null ||
        !_cameraController!.value.isInitialized ||
        _lastResults == null ||
        _lastResults!.isEmpty ||
        _isCapturing) {
      return;
    }

    setState(() => _isCapturing = true);

    try {
      // 1. Ch·ª•p ·∫£nh
      final image = await _cameraController!.takePicture();

      // 2. Load ·∫£nh ƒë·ªÉ crop
      final bytes = await File(image.path).readAsBytes();
      final originalImage = img.decodeImage(bytes);

      if (originalImage == null) {
        throw Exception('Kh√¥ng th·ªÉ decode ·∫£nh');
      }

      print('üì∏ Original image size: ${originalImage.width}x${originalImage.height}');

      // 3. L·∫•y bounding box c·ªßa face ƒë·∫ßu ti√™n
      final firstFace = _lastResults!.first;
      final box = firstFace.boundingBox;

      print('üì¶ Box from YOLO (preview coords): ${box.left}, ${box.top}, ${box.width}, ${box.height}');

      // 4. ‚úÖ QUAN TR·ªåNG: T√≠nh t·ª∑ l·ªá scale gi·ªØa ·∫£nh ch·ª•p v√† preview
      // Preview size t·ª´ YOLOView (th∆∞·ªùng l√† 480x640 ho·∫∑c t√πy device)
      // B·∫°n c·∫ßn l·∫•y preview size th·ª±c t·∫ø, t·∫°m th·ªùi d√πng gi√° tr·ªã ∆∞·ªõc l∆∞·ª£ng
      final previewWidth = 640.0;
      final previewHeight = 480.0;

      final scaleX = originalImage.width / previewHeight; // Scale X c·ªßa ·∫£nh (720) v·ªõi Y c·ªßa stream (480)
      final scaleY = originalImage.height / previewWidth; // Scale Y c·ªßa ·∫£nh (1280) v·ªõi X c·ªßa stream (640)

      print('üìê Scale factors: scaleX=$scaleX, scaleY=$scaleY');

      // 5. Scale bounding box l√™n k√≠ch th∆∞·ªõc ·∫£nh th·ª±c
      final padding = -30.0;

      final scaledLeft = (box.left * scaleX - padding).clamp(0.0, originalImage.width.toDouble());
      final scaledTop = (box.top * scaleY - padding).clamp(0.0, originalImage.height.toDouble());
      final scaledWidth = (box.width * scaleX + padding * 2).clamp(0.0, originalImage.width - scaledLeft);
      final scaledHeight = (box.height * scaleY + padding * 2).clamp(0.0, originalImage.height - scaledTop);

      final x = scaledLeft.toInt();
      final y = scaledTop.toInt();
      final w = scaledWidth.toInt();
      final h = scaledHeight.toInt();

      print('‚úÇÔ∏è Crop coords: x=$x, y=$y, w=$w, h=$h');

      // 6. Crop khu√¥n m·∫∑t
      final croppedFace = img.copyCrop(
        originalImage,
        x: x,
        y: y,
        width: w,
        height: h,
      );

      print('‚úÖ Cropped face size: ${croppedFace.width}x${croppedFace.height}');

      // 7. L∆∞u ·∫£nh ƒë√£ crop
      final tempDir = await getTemporaryDirectory();
      final facePath = '${tempDir.path}/face_${DateTime.now().millisecondsSinceEpoch}.jpg';
      await File(facePath).writeAsBytes(img.encodeJpg(croppedFace));

      print('üíæ Saved cropped face to: $facePath');

      // 8. Chuy·ªÉn sang m√†n h√¨nh ph√¢n t√≠ch c·∫£m x√∫c
      if (mounted) {
        await Navigator.push(
          context,
          MaterialPageRoute(
            builder: (context) => EmotionResultScreen(
              faceImagePath: facePath,
            ),
          ),
        );
      }

    } catch (e) {
      print('‚ùå Error capturing face: $e');
      if (mounted) {
        ScaffoldMessenger.of(context).showSnackBar(
          SnackBar(content: Text('L·ªói: $e')),
        );
      }
    } finally {
      setState(() => _isCapturing = false);
    }
  }

  @override
  void dispose() {
    _cameraController?.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: Text('Face Detection ($_count)'),
        backgroundColor: Colors.black,
      ),
      body: Stack(
        children: [
          /// Camera + YOLO Detection
          YOLOView(
            modelPath: 'model_face.tflite',
            task: YOLOTask.detect,
            confidenceThreshold: 0.5,
            iouThreshold: 0.7,
            useGpu: true,
            onResult: (results) {
              if (results == null || results.isEmpty) {
                setState(() {
                  _count = 0;
                  _info = '';
                  _lastResults = null;
                });
                return;
              }

              print('üîç Detection results: $results');
              print('üìä Found ${results.length} faces');

              setState(() {
                _count = results.length;
                _lastResults = results;
                _info = results
                    .map((r) =>
                '${r.className ?? 'face'}: ${(r.confidence * 100).toStringAsFixed(1)}%')
                    .join('\n');
              });
            },
          ),

          /// Info Overlay
          Positioned(
            bottom: 100,
            left: 20,
            right: 20,
            child: Container(
              padding: const EdgeInsets.all(16),
              decoration: BoxDecoration(
                color: Colors.black87,
                borderRadius: BorderRadius.circular(12),
                border: Border.all(color: Colors.blueAccent, width: 2),
              ),
              child: Column(
                mainAxisSize: MainAxisSize.min,
                children: [
                  Text(
                    'Ph√°t hi·ªán: $_count khu√¥n m·∫∑t',
                    style: const TextStyle(
                      color: Colors.white,
                      fontSize: 18,
                      fontWeight: FontWeight.bold,
                    ),
                  ),
                  if (_info.isNotEmpty) ...[
                    const SizedBox(height: 8),
                    Text(
                      _info,
                      style: const TextStyle(color: Colors.white70),
                      textAlign: TextAlign.center,
                    ),
                  ],
                ],
              ),
            ),
          ),

          /// Capture Button
          Positioned(
            bottom: 20,
            left: 0,
            right: 0,
            child: Center(
              child: FloatingActionButton.extended(
                onPressed: _count > 0 && !_isCapturing ? _captureAndAnalyzeFace : null,
                backgroundColor: _count > 0 ? Colors.blue : Colors.grey,
                icon: _isCapturing
                    ? const SizedBox(
                  width: 20,
                  height: 20,
                  child: CircularProgressIndicator(
                    color: Colors.white,
                    strokeWidth: 2,
                  ),
                )
                    : const Icon(Icons.camera_alt),
                label: Text(_isCapturing ? 'ƒêang x·ª≠ l√Ω...' : 'Ph√¢n t√≠ch c·∫£m x√∫c'),
              ),
            ),
          ),
        ],
      ),
    );
  }
}